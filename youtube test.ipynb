{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the google-api-python-client, pymongo, psycopg2, and isodate packages\n",
    "%pip install google-api-python-client pymongo psycopg2-binary isodate\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "import pymongo\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import isodate\n",
    "from datetime import timedelta\n",
    "import streamlit as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API key connection\n",
    "def Api_connect():\n",
    "    api_ID = 'AIzaSyAyUD9VnxSAs6VDHDr9Q8vBYEQJCU0vApg'\n",
    "    api_service_name = 'youtube'\n",
    "    api_version = 'v3'\n",
    "    youtube = build(api_service_name, api_version, developerKey=api_ID)\n",
    "    return youtube\n",
    "youtube = Api_connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert =Channel_full_details('UChfjFIaQtd514PWnPzfj8xg')\n",
    "# print(insert)\n",
    "# Channel_ID = \n",
    "# Richtheman     - UCWWbC9jF2IOoww4lzhK4HFQ\n",
    "# SonyEurope     - UCsmJ-6uyV8HDxO0BmPWk-kQ\n",
    "# up&atom        - UCSIvk78tK2TiviLQn4fSHaw\n",
    "# Kwella         - UCDY_0WzkHyj0A1ev6RTql1Q\n",
    "# Brooke Glaser  - UChfjFIaQtd514PWnPzfj8xg\n",
    "# Quantamagazine - UCTpmmkp1E4nmZqWPS-dl5bg\n",
    "# TOHO animation - UC14Yc2Qv92DMuyNRlHvpo2Q\n",
    "# Mappa          - UCjfAEJZdfbIjVHdo5yODfyQ\n",
    "# Disney         - UCgwv23FVv3lqh567yagXfNg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get channel information\n",
    "def get_channel_info(channel_id):\n",
    "      request = youtube.channels().list(part=\"snippet,ContentDetails,statistics\",id = channel_id)\n",
    "      response = request.execute()\n",
    "\n",
    "      for i in response['items']:\n",
    "            data = dict(Channel_Name = i['snippet']['title'],\n",
    "                  Channel_ID = i['id'],\n",
    "                  Channel_Description = i['snippet']['description'],\n",
    "                  Channel_Creation_Date = i['snippet']['publishedAt'],\n",
    "                  Channel_Thumbnail = i['snippet']['thumbnails']['default']['url'],\n",
    "                  Channel_View_Count = i['statistics']['viewCount'],\n",
    "                  Channel_Subscriber_Count = i['statistics']['subscriberCount'],\n",
    "                  Channel_Video_Count = i['statistics']['videoCount'],\n",
    "                  Playlist_ID = i['contentDetails']['relatedPlaylists']['uploads'])\n",
    "      return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get video ID:\n",
    "def get_video_ids(channel_id):\n",
    "\n",
    "    video_ids =[]\n",
    "    response = youtube.channels().list(id = channel_id,\n",
    "                                        part='contentDetails').execute()\n",
    "    playlist_id = response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        response1 = youtube.playlistItems().list(part='contentDetails',playlistId = playlist_id,maxResults =50,pageToken=next_page_token).execute()\n",
    "        \n",
    "        for i in response1['items']:\n",
    "            video_ids.append(i['contentDetails']['videoId'])\n",
    "        next_page_token = response1.get('nextPageToken')\n",
    "        \n",
    "        if next_page_token is None:\n",
    "            break\n",
    "    return video_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_ids 'E58hwj_yZNI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get video information\n",
    "def get_video_info(video_ids):\n",
    "        Video_data = []\n",
    "\n",
    "        for video_id in video_ids:\n",
    "                request = youtube.videos().list(part=\"snippet,ContentDetails,statistics\",id = video_id)\n",
    "                response = request.execute()\n",
    "\n",
    "                for item in response['items']:\n",
    "                        data = dict(Channel_name = item['snippet']['channelTitle'],\n",
    "                                Channel_ID = item['snippet']['channelId'],\n",
    "                                Video_ID = item['id'],\n",
    "                                Video_Title = item['snippet']['title'],\n",
    "                                Video_Description = item['snippet'].get('description'),\n",
    "                                Video_Publish_Date = item['snippet']['publishedAt'],\n",
    "                                Video_Duration = item['contentDetails']['duration'],\n",
    "                                Video_tags = item['snippet'].get('tags'),\n",
    "                                Video_Thumbnail = item['snippet']['thumbnails']['default']['url'],\n",
    "                                Video_View_Count = item['statistics'].get('viewCount'),\n",
    "                                Video_Like_Count = item['statistics'].get('likeCount'),\n",
    "                                Video_favorite_count = item['statistics'].get('favoriteCount'),\n",
    "                                Video_Comment_Count = item['statistics'].get('commentCount'),\n",
    "                                Video_definition = item['contentDetails']['definition'],\n",
    "                                Video_Caption = item['contentDetails']['caption'],)\n",
    "                        Video_data.append(data)\n",
    "        return Video_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_comment_info:\n",
    "from googleapiclient.errors import HttpError  \n",
    "\n",
    "def get_comment_info(video_ids):\n",
    "    try:\n",
    "        comment_data = []\n",
    "        for video_id in video_ids:\n",
    "            \n",
    "            video_request = youtube.videos().list(part=\"snippet\", id=video_id)\n",
    "            video_response = video_request.execute()\n",
    "            \n",
    "            if \"items\" in video_response and len(video_response[\"items\"]) > 0:\n",
    "                video_title = video_response[\"items\"][0][\"snippet\"][\"title\"]\n",
    "            else:\n",
    "                video_title = \"Unknown Title\"\n",
    "            \n",
    "            try:\n",
    "                comment_request = youtube.commentThreads().list(part=\"snippet\", videoId=video_id, maxResults=100)\n",
    "                comment_response = comment_request.execute()\n",
    "\n",
    "                for item in comment_response.get(\"items\", []): \n",
    "                    snippet = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "                    data = dict(\n",
    "                        Video_ID=snippet[\"videoId\"],\n",
    "                        Video_Title=video_title,  # Include the video title\n",
    "                        comment_id=item[\"snippet\"][\"topLevelComment\"][\"id\"],\n",
    "                        comment_text=snippet[\"textDisplay\"],\n",
    "                        comment_author=snippet[\"authorDisplayName\"],\n",
    "                        comment_like_count=snippet[\"likeCount\"],\n",
    "                        comment_publish_date=snippet[\"publishedAt\"]\n",
    "                    )\n",
    "                    comment_data.append(data)\n",
    "            except HttpError as e:\n",
    "                if e.resp.status == 403:\n",
    "                    print(f\"Comments are disabled for video ID: {video_id}. Skipping.\")\n",
    "                else:\n",
    "                    print(f\"An unexpected error occurred for video ID {video_id}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "    return comment_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_playlist_info:\n",
    "\n",
    "def get_playlist_info(channel_id):\n",
    "    next_page_token = None\n",
    "    playlist_data = []\n",
    "\n",
    "    while True:\n",
    "        request = youtube.playlists().list(part = 'snippet,contentDetails',channelId = channel_id ,maxResults = 50,pageToken = next_page_token)\n",
    "        response = request.execute() \n",
    "\n",
    "        for item in response['items']:\n",
    "            data = dict(Playlist_ID = item['id'],\n",
    "                        Playlist_Title = item['snippet']['title'],\n",
    "                        Playlist_Description = item['snippet']['description'],\n",
    "                        Playlist_Publish_Date = item['snippet']['publishedAt'],\n",
    "                        Playlist_Channel_ID = item['snippet']['channelId'],\n",
    "                        Playlist_Channel_Title = item['snippet']['channelTitle'],\n",
    "                        Playlist_Video_Count = item['contentDetails']['itemCount'])\n",
    "            playlist_data.append(data)\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if next_page_token is None:\n",
    "            break\n",
    "    return playlist_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb+srv://Kenny_Ai:kenny_Ai@cluster0.mh35r.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\")\n",
    "db = client['Youtube']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting data into the collection\n",
    "def Channel_full_details(channel_id):\n",
    "    ch_details = get_channel_info(channel_id)\n",
    "    pl_details = get_playlist_info(channel_id)\n",
    "    video_ids = get_video_ids(channel_id)\n",
    "    video_info = get_video_info(video_ids)\n",
    "    comment_info = get_comment_info(video_ids)\n",
    "    col1 = db['Channel_Details']\n",
    "    col1.insert_one({'Channel_Details':ch_details, 'Playlist_Details':pl_details, 'Video_Details':video_info, 'Comment_Details':comment_info})\n",
    "    return 'upload completed successfully'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert =Channel_full_details('UChfjFIaQtd514PWnPzfj8xg')\n",
    "# print(insert)\n",
    "# Channel_ID = \n",
    "# Richtheman     - UCWWbC9jF2IOoww4lzhK4HFQ\n",
    "# SonyEurope     - UCsmJ-6uyV8HDxO0BmPWk-kQ\n",
    "# up&atom        - UCSIvk78tK2TiviLQn4fSHaw\n",
    "# Kwella         - UCDY_0WzkHyj0A1ev6RTql1Q\n",
    "# Brooke Glaser  - UChfjFIaQtd514PWnPzfj8xg\n",
    "# Quantamagazine - UCTpmmkp1E4nmZqWPS-dl5bg\n",
    "# TOHO animation - UC14Yc2Qv92DMuyNRlHvpo2Q\n",
    "# Mappa          - UCjfAEJZdfbIjVHdo5yODfyQ\n",
    "# Disney         - UCgwv23FVv3lqh567yagXfNg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for table creation for Channels in postgresql\n",
    "def channels_table(channel_name_single):\n",
    "    mydb = psycopg2.connect(host = \"localhost\",user= \"postgres\",password = \"root\",database = \"Youtube_data_harvesting\",port = \"5432\")\n",
    "    cursor = mydb.cursor()\n",
    "\n",
    "    \n",
    "    create_query = '''create table if not exists channels( channel_name varchar(100), channel_id varchar(100) primary key,\n",
    "                                    channel_description text, channel_creation_date date, channel_thumbnail text, \n",
    "                                    channel_view_count bigint, channel_subscriber_count int, channel_video_count int, playlist_id varchar(100));'''\n",
    "    cursor.execute(create_query)\n",
    "    mydb.commit()\n",
    "    \n",
    "\n",
    "    single_channel_detail = []\n",
    "    col1 = db['Channel_Details']\n",
    "    for ch_data in col1.find({'Channel_Details.Channel_Name': channel_name_single},{'_id':0,'Channel_Details':1}):\n",
    "        single_channel_detail.append(ch_data[\"Channel_Details\"])\n",
    "\n",
    "    df_single_channel_detail = pd.DataFrame(single_channel_detail)\n",
    "\n",
    "\n",
    "\n",
    "    for index, row in df_single_channel_detail.iterrows():\n",
    "        insert_query = '''insert into channels(channel_name,channel_id,channel_description,channel_creation_date,\n",
    "                                            channel_thumbnail,channel_view_count,channel_subscriber_count,\n",
    "                                            channel_video_count,playlist_id) \n",
    "                                            values(%s,%s,%s,%s,%s,%s,%s,%s,%s);'''\n",
    "        \n",
    "        row['Channel_View_Count'] = min(int(row['Channel_View_Count']), 2147483647)\n",
    "        values = (row['Channel_Name'], row['Channel_ID'], row['Channel_Description'], row['Channel_Creation_Date'],\n",
    "                row['Channel_Thumbnail'], row['Channel_View_Count'], row['Channel_Subscriber_Count'],\n",
    "                row['Channel_Video_Count'], row['Playlist_ID'])\n",
    "        try:\n",
    "            cursor.execute(insert_query, values)\n",
    "            mydb.commit()\n",
    "        except:\n",
    "            news = f'The provided channel name {channel_name_single} already exists in the database'\n",
    "            return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table creation for playlist in postgresql\n",
    "def playlist_table(channel_name_single):\n",
    "    mydb = psycopg2.connect(host = \"localhost\",user= \"postgres\",password = \"root\",database = \"Youtube_data_harvesting\",port = \"5432\")\n",
    "    cursor = mydb.cursor()\n",
    "\n",
    "    create_query = '''create table if not exists playlists(playlist_id varchar(100) primary key,\n",
    "                                    playlist_title varchar(100), playlist_description text, playlist_publish_date timestamp,\n",
    "                                    playlist_channel_id varchar(100), playlist_channel_title varchar(100), playlist_video_count int);'''\n",
    "    cursor.execute(create_query)\n",
    "    mydb.commit()\n",
    "\n",
    "    single_playlist_details =[]\n",
    "    db = client['Youtube']\n",
    "    col1 = db['Channel_Details']\n",
    "    for pl_data in col1.find({'Channel_Details.Channel_Name': channel_name_single},{'_id':0,'Playlist_Details':1}):\n",
    "        single_playlist_details.extend(pl_data[\"Playlist_Details\"])\n",
    "\n",
    "    df_single_channel_playlist_detail = pd.DataFrame(single_playlist_details)\n",
    "\n",
    "    for index, row in  df_single_channel_playlist_detail.iterrows():\n",
    "        insert_query = '''insert into playlists(playlist_id,playlist_title,playlist_description,playlist_publish_date,\n",
    "                                            playlist_channel_id,playlist_channel_title,playlist_video_count) \n",
    "                                            values(%s,%s,%s,%s,%s,%s,%s);'''\n",
    "        values = (row['Playlist_ID'], row['Playlist_Title'], row['Playlist_Description'],\n",
    "                row['Playlist_Publish_Date'],row['Playlist_Channel_ID'], row['Playlist_Channel_Title'],\n",
    "                row['Playlist_Video_Count'])\n",
    "        cursor.execute(insert_query, values)\n",
    "        mydb.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table creation for videos in postgresql\n",
    "def create_table_videos(channel_name_single):\n",
    "    mydb = psycopg2.connect(host = \"localhost\",user= \"postgres\",password = \"root\",database = \"Youtube_data_harvesting\",port = \"5432\")\n",
    "    cursor = mydb.cursor()\n",
    "   \n",
    "    create_query = '''create table if not exists videos(video_id varchar(100) primary key,channel_name varchar(100),channel_id varchar(100),\n",
    "    video_title varchar(100),video_description text,video_publish_date date,video_duration time,video_tags text,video_thumbnail text,\n",
    "    video_view_count bigint,video_like_count int,video_favorite_count int,video_comment_count int,video_definition varchar(100),\n",
    "    video_caption varchar(100));'''\n",
    "    cursor.execute(create_query)\n",
    "    mydb.commit()\n",
    "    \n",
    "    single_video_details =[]\n",
    "    db = client['Youtube']\n",
    "    col1 = db['Channel_Details']\n",
    "    for video_data in col1.find({'Channel_Details.Channel_Name': channel_name_single},{'_id':0,'Video_Details':1}):\n",
    "        single_video_details.extend(video_data[\"Video_Details\"])\n",
    "    df_single_video_details = pd.DataFrame(single_video_details)\n",
    "\n",
    "    def parse_duration(duration):\n",
    "        try:\n",
    "            parsed_duration = isodate.parse_duration(duration)  # Parses 'PT43S' to timedelta\n",
    "            total_seconds = int(parsed_duration.total_seconds())\n",
    "            formatted_duration = str(timedelta(seconds=total_seconds))  # Converts to 'HH:MM:SS'\n",
    "            return formatted_duration\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing duration: {duration} - {e}\")\n",
    "            return None\n",
    "        \n",
    "    df_single_video_details['Video_Duration'] = df_single_video_details['Video_Duration'].apply(parse_duration)\n",
    "\n",
    "    df_single_video_details['Video_View_Count'] = df_single_video_details['Video_View_Count'].apply(lambda x: min(int(x), 2147483647))\n",
    "\n",
    "    for index, row in df_single_video_details.iterrows():\n",
    "        insert_query = '''\n",
    "        insert into videos(video_id,channel_name,channel_id,video_title,video_description,video_publish_date,\n",
    "        video_duration,video_tags,video_thumbnail,video_view_count,video_like_count,video_favorite_count,\n",
    "        video_comment_count,video_definition,video_caption) \n",
    "        values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s);'''\n",
    "\n",
    "        row['Video_View_Count'] = min(int(row['Video_View_Count']), 2147483647)\n",
    "        values = (row['Video_ID'], row['Channel_name'], row['Channel_ID'], row['Video_Title'], row['Video_Description'],\n",
    "                row['Video_Publish_Date'], row['Video_Duration'], row['Video_tags'], row['Video_Thumbnail'],\n",
    "                row['Video_View_Count'], row['Video_Like_Count'], row['Video_favorite_count'], row['Video_Comment_Count'],\n",
    "                row['Video_definition'], row['Video_Caption'])\n",
    "        \n",
    "        cursor.execute(insert_query, values)\n",
    "        mydb.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table creation for comments in postgresql\n",
    "def comments_table(channel_name_single):\n",
    "    # Connect to PostgreSQL database\n",
    "    mydb = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"postgres\",\n",
    "        password=\"root\",\n",
    "        database=\"Youtube_data_harvesting\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    cursor = mydb.cursor()\n",
    "\n",
    "    create_query = '''\n",
    "    CREATE TABLE IF NOT EXISTS comments(\n",
    "        video_id VARCHAR(100),\n",
    "        video_title VARCHAR(255),\n",
    "        comment_id VARCHAR(100) PRIMARY KEY,\n",
    "        comment_text TEXT,\n",
    "        comment_author VARCHAR(100),\n",
    "        comment_like_count INT,\n",
    "        comment_publish_date TIMESTAMP\n",
    "    );\n",
    "    '''\n",
    "    cursor.execute(create_query)\n",
    "    mydb.commit()\n",
    "\n",
    "    # Extract comment details from MongoDB\n",
    "    single_comment_details = []\n",
    "    db = client['Youtube']\n",
    "    col1 = db['Channel_Details']\n",
    "\n",
    "    # Fetch comment details and video title\n",
    "    for comm_data in col1.find(\n",
    "        {'Channel_Details.Channel_Name': channel_name_single}, \n",
    "        {'_id': 0, 'Comment_Details': 1, 'Video_Details': 1}\n",
    "    ):\n",
    "        single_comment_details.extend(comm_data[\"Comment_Details\"])\n",
    "    \n",
    "    df_single_comment_details = pd.DataFrame(single_comment_details)\n",
    "\n",
    "    # Add Video_Title to the DataFrame\n",
    "    video_details = {}\n",
    "    for video in col1.find({'Channel_Details.Channel_Name': channel_name_single}, {'_id': 0, 'Video_Details': 1}):\n",
    "        for vid in video[\"Video_Details\"]:\n",
    "            video_details[vid[\"Video_ID\"]] = vid[\"Video_Title\"]\n",
    "    \n",
    "    df_single_comment_details['Video_Title'] = df_single_comment_details['Video_ID'].map(video_details)\n",
    "\n",
    "    # Insert data into the comments table\n",
    "    for index, row in df_single_comment_details.iterrows():\n",
    "        insert_query = '''\n",
    "        INSERT INTO comments(\n",
    "            video_id,\n",
    "            video_title,\n",
    "            comment_id,\n",
    "            comment_text,\n",
    "            comment_author,\n",
    "            comment_like_count,\n",
    "            comment_publish_date\n",
    "        ) VALUES (%s, %s, %s, %s, %s, %s, %s);\n",
    "        '''\n",
    "        values = (\n",
    "            row['Video_ID'],\n",
    "            row['Video_Title'],\n",
    "            row['comment_id'],\n",
    "            row['comment_text'],\n",
    "            row['comment_author'],\n",
    "            row['comment_like_count'],\n",
    "            row['comment_publish_date']\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            cursor.execute(insert_query, values)\n",
    "            mydb.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error inserting row {index}: {e}\")\n",
    "\n",
    "    cursor.close()\n",
    "    mydb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tables(single_channel):\n",
    "    news = channels_table(single_channel)\n",
    "    if news:\n",
    "        return news\n",
    "    else:\n",
    "        playlist_table(single_channel)\n",
    "        create_table_videos(single_channel)\n",
    "        comments_table(single_channel)\n",
    "        return 'Tables created successfully'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_channel_table():\n",
    "    Ch_list =[]\n",
    "    db = client['Youtube']\n",
    "    col1 = db['Channel_Details']\n",
    "    for ch_data in col1.find({},{'_id':0,'Channel_Details':1}):\n",
    "        Ch_list.append(ch_data[\"Channel_Details\"])\n",
    "\n",
    "    df = st.dataframe(Ch_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_playlist_table():\n",
    "    Pl_list =[]\n",
    "    db = client['Youtube']\n",
    "    col1 = db['Channel_Details']\n",
    "    for pl_data in col1.find({},{'_id':0,'Playlist_Details':1}):\n",
    "        Pl_list.extend(pl_data[\"Playlist_Details\"])\n",
    "\n",
    "    df1 = st.dataframe(Pl_list)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_video_table():\n",
    "  Video_list =[]\n",
    "  db = client['Youtube']\n",
    "  col1 = db['Channel_Details']\n",
    "  for video_data in col1.find({},{'_id':0,'Video_Details':1}):\n",
    "        Video_list.extend(video_data[\"Video_Details\"])\n",
    "\n",
    "  df3 = st.dataframe(Video_list)\n",
    "  return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comment_table():\n",
    "    Comm_list =[]\n",
    "    db = client['Youtube']\n",
    "    col1 = db['Channel_Details']\n",
    "    for comm_data in col1.find({},{'_id':0,'Comment_Details':1}):\n",
    "        Comm_list.extend(comm_data[\"Comment_Details\"])\n",
    "        \n",
    "    df2 = st.dataframe(Comm_list)\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Streamlit code\n",
    "st.markdown(\"\"\"<h1 style=\"font-size:35px;\">Youtube Data Harvesting & Warehousing</h1>\"\"\",unsafe_allow_html=True,)\n",
    "st.write(\"This app harvests data from Youtube channels and stores it in a MongoDB database. It then extracts the data from the MongoDB database and stores it in a PostgreSQL database.\")\n",
    "\n",
    "with st.sidebar:\n",
    "    st.write(\"### Instructions\")\n",
    "    st.write(\"1. Enter the channel ID in the text box.\")\n",
    "    st.write(\"2. Click the 'Harvest Data' button to start the data harvesting process.\")\n",
    "    st.write(\"3. Click the 'Create Tables' button to create the tables in the PostgreSQL database.\")\n",
    "    st.write(\"4. Click the 'Display Tables' button to view the data in the tables.\")\n",
    "    st.write(\"5. Click the 'Clear Data' button to delete all data from the MongoDB and PostgreSQL databases.\")\n",
    "\n",
    "channel_id = st.text_input(\"Enter the channel ID:\")\n",
    "\n",
    "# MongoDB connection\n",
    "client = pymongo.MongoClient(\"mongodb+srv://Kenny_Ai:kenny_Ai@cluster0.mh35r.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\")\n",
    "db = client['Youtube']\n",
    "\n",
    "# SQL connection\n",
    "mydb = psycopg2.connect(host = \"localhost\",user= \"postgres\",password = \"root\",database = \"Youtube_data_harvesting\",port = \"5432\")\n",
    "cursor = mydb.cursor()\n",
    "\n",
    "if st.button(\"Harvest and store Data\"):\n",
    "    Ch_list =[]\n",
    "    col1 = db['Channel_Details']\n",
    "    for ch_data in col1.find({},{'_id':0,'Channel_Details':1}):\n",
    "        Ch_list.append(ch_data[\"Channel_Details\"][\"Channel_ID\"])\n",
    "    if channel_id in Ch_list:\n",
    "        st.write(\"Data already exists in the database.\")\n",
    "    else:\n",
    "        insert = Channel_full_details(channel_id)\n",
    "        st.success(insert)\n",
    "\n",
    "All_channels_list =[]\n",
    "db = client['Youtube']\n",
    "col1 = db['Channel_Details']\n",
    "for ch_data in col1.find({},{'_id':0,'Channel_Details':1}):\n",
    "    All_channels_list.append(ch_data[\"Channel_Details\"][\"Channel_Name\"])\n",
    "\n",
    "unique_channel = st.selectbox(\"Select the channel\", All_channels_list)\n",
    "    \n",
    "if st.button(\"Migrate data to PostgreSQL\"):\n",
    "    SQL_tables = tables(unique_channel)\n",
    "    st.success(SQL_tables)\n",
    "\n",
    "if st.button(\"Clear Data\"):\n",
    "    # Drop PostgreSQL tables\n",
    "    drop_queries = [\n",
    "        '''drop table if exists comments;''',\n",
    "        '''drop table if exists videos;''',\n",
    "        '''drop table if exists playlists;''',\n",
    "        '''drop table if exists channels;'''\n",
    "    ]\n",
    "\n",
    "    for drop_query in drop_queries:\n",
    "        try:\n",
    "            cursor.execute(drop_query)\n",
    "            mydb.commit()\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error dropping table: {e}\")\n",
    "            mydb.rollback()\n",
    "    # Drop MongoDB database\n",
    "    db_name = \"Youtube\"\n",
    "    try:\n",
    "        if db_name in client.list_database_names():\n",
    "            client.drop_database(db_name)\n",
    "            st.success(f\"Database '{db_name}' has been deleted.\")\n",
    "        else:\n",
    "            st.info(f\"Database '{db_name}' does not exist.\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error deleting MongoDB database: {e}\")\n",
    "    # Clear Streamlit session state\n",
    "    for key in list(st.session_state.keys()):\n",
    "        del st.session_state[key]\n",
    "\n",
    "    st.success(\"All data cleared successfully!\")\n",
    "\n",
    "\n",
    "show_tables = st.radio(\"Select the table you want to view:\", (\"Channel\", \"Playlist\", \"Video\", \"Comment\"))\n",
    "if show_tables == \"Channel\":\n",
    "    display_channel_table()\n",
    "elif show_tables == \"Playlist\":\n",
    "    display_playlist_table()\n",
    "elif show_tables == \"Video\":\n",
    "    display_video_table()\n",
    "else:\n",
    "    display_comment_table()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 19:39:29.427 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-06 19:39:29.428 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-06 19:39:29.429 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-06 19:39:29.430 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-06 19:39:29.431 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-06 19:39:29.432 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# SQL connection\n",
    "mydb = psycopg2.connect(host = \"localhost\",user= \"postgres\",password = \"root\",database = \"Youtube_data_harvesting\",port = \"5432\")\n",
    "cursor = mydb.cursor()\n",
    "\n",
    "question = st.selectbox(\"Select your question:\", (\n",
    "                                                  \"1. The names of all the videos and their corresponding channels.\", \n",
    "                                                  \"2. Channels that have the most number of videos and their total number of videos.\",\n",
    "                                                  \"3. The top 10 most viewed videos and their respective channels.\",\n",
    "                                                  \"4. The comments count made on each video and their corresponding video names.\",\n",
    "                                                  \"5. The videos which have the highest number of likes and their corresponding channel names.\",\n",
    "                                                  \"6. The total number of likes for each video and their corresponding video names.\",\n",
    "                                                  \"7. The total number of views for each channel, and what are their corresponding channel names.\",\n",
    "                                                  \"8. The names of all the channels that have published videos in the year 2022.\",\n",
    "                                                  \"9. The average duration of all videos in each channel and their corresponding channel names.\",\n",
    "                                                  \"10. The videos that have the highest number of comments and their corresponding channel names\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 19:39:29.507 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-01-06 19:39:29.507 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Sql queries with answers\n",
    "\n",
    "mydb = psycopg2.connect(host = \"localhost\",user= \"postgres\",password = \"root\",database = \"Youtube_data_harvesting\",port = \"5432\")\n",
    "cursor = mydb.cursor()\n",
    "\n",
    "if question == \"1. The names of all the videos and their corresponding channels\":\n",
    "    query1 = '''select video_title,channel_name from videos;'''\n",
    "    cursor.execute(query1)\n",
    "    mydb.commit()\n",
    "    result1 = cursor.fetchall()\n",
    "    df = pd.DataFrame(result1, columns = ['Video Title', 'Channel Name'])\n",
    "    st.write(df)\n",
    "\n",
    "elif question == \"2. Channels that have the most number of videos and their total number of videos.\":\n",
    "    query2 = '''select channel_name,channel_view_count from channels order by channel_view_count desc;'''\n",
    "    cursor.execute(query2)\n",
    "    mydb.commit()\n",
    "    result2 = cursor.fetchall()\n",
    "    df = pd.DataFrame(result2, columns = ['Channel Name', 'Channel View Count'])\n",
    "    st.write(df)\n",
    "\n",
    "elif question == \"3. The top 10 most viewed videos and their respective channels.\":\n",
    "    query3 = '''select video_title,video_view_count,channel_name from videos order by video_view_count desc limit 10;'''\n",
    "    cursor.execute(query3)\n",
    "    mydb.commit()\n",
    "    result3 = cursor.fetchall()\n",
    "    df = pd.DataFrame(result3, columns = ['Video Title', 'Video View Count','Channel Name'])\n",
    "    st.write(df)\n",
    "\n",
    "elif question == \"4. The comments count made on each video and their corresponding video names.\":\n",
    "    query4 = '''\n",
    "    SELECT video_title, COUNT(comment_id) AS number_of_comments\n",
    "    FROM comments\n",
    "    GROUP BY video_title\n",
    "    ORDER BY number_of_comments DESC;\n",
    "    '''\n",
    "    cursor.execute(query4)\n",
    "    result4 = cursor.fetchall()  # No need to commit for SELECT queries\n",
    "    df = pd.DataFrame(result4, columns=['Video Title', 'Number of Comments'])\n",
    "    st.write(df)\n",
    "\n",
    "\n",
    "elif question == \"5. The videos which have the highest number of likes and their corresponding channel names.\":\n",
    "    query5 = '''\n",
    "    SELECT channel_name, video_title, video_like_count\n",
    "    FROM videos\n",
    "    ORDER BY video_like_count DESC;\n",
    "    '''\n",
    "    cursor.execute(query5)\n",
    "    result5 = cursor.fetchall()  # No need to commit for SELECT queries\n",
    "    df = pd.DataFrame(result5, columns=['Channel Name', 'Video Title', 'Video Like Count'])\n",
    "    st.write(df)\n",
    "\n",
    "\n",
    "elif question == \"6. The total number of likes for each video and their corresponding video names.\":\n",
    "    query6 = '''select video_title,video_like_count from videos;'''\n",
    "    cursor.execute(query6)\n",
    "    mydb.commit()\n",
    "    result6 = cursor.fetchall()\n",
    "    df = pd.DataFrame(result6, columns = ['Video Title', 'Video Like Count'])\n",
    "    st.write(df)\n",
    "\n",
    "elif question == \"7. The total number of views for each channel, and what are their corresponding channel names.\":\n",
    "    query7 = '''select channel_name,sum(video_view_count) as total_views from videos group by channel_name;'''\n",
    "    cursor.execute(query7)\n",
    "    mydb.commit()\n",
    "    result7 = cursor.fetchall()\n",
    "    df = pd.DataFrame(result7, columns = ['Channel Name', 'Total Views'])\n",
    "    st.write(df)\n",
    "\n",
    "elif question == \"8. The names of all the channels that have published videos in the year 2022.\":\n",
    "    query8 = '''SELECT channel_name, video_title, video_publish_date \n",
    "                FROM videos \n",
    "                WHERE EXTRACT(year FROM video_publish_date) = 2022;'''\n",
    "    cursor.execute(query8)\n",
    "    result8 = cursor.fetchall() \n",
    "    df = pd.DataFrame(result8, columns = ['Channel Name', 'Video Title', 'Video Publish Date'])\n",
    "    st.write(df)\n",
    "\n",
    "\n",
    "elif question == \"9. The average duration of all videos in each channel and their corresponding channel names.\":\n",
    "    query9 = '''SELECT channel_name, \n",
    "                       AVG(EXTRACT(epoch FROM video_duration) / 60) AS average_duration_minutes \n",
    "                FROM videos \n",
    "                GROUP BY channel_name;'''\n",
    "    cursor.execute(query9)\n",
    "    result9 = cursor.fetchall()\n",
    "    df = pd.DataFrame(result9, columns = ['Channel Name', 'Average Duration (Minutes)'])\n",
    "    st.write(df)\n",
    "\n",
    "else:\n",
    "    query10 = '''select video_title as videotitle, channel_name as channelname, video_comment_count as videocommentcount from videos \n",
    "                where video_comment_count is not null order by video_comment_count desc;'''\n",
    "    cursor.execute(query10)\n",
    "    mydb.commit()\n",
    "    result10 = cursor.fetchall()\n",
    "    df = pd.DataFrame(result10, columns =  ['Video Title', 'Channel Name ','Video Comment Count'])\n",
    "    st.write(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
